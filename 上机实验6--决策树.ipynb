{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上机实验8：决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务1：分支节点的选择方法\n",
    "\n",
    "现有一个数据集 weekend.txt，目标是根据一个人的特征来预测其周末是否出行。\n",
    "\n",
    "所有特征均为二元特征，取值为 0 或 1，其中“status”（目标特征也是类别）表示用户的周末是否出行，1 表示出行，0 表示不出行，“marriageStatus”表示申请人是否已婚、“hasChild”表示申请人是否有小孩、“hasAppointment”表示申请人是否有约、“weather”表示天气是否晴朗。\n",
    "\n",
    "已知信息熵和信息增益的公式为：\n",
    "\n",
    "$$\\text{Entropy}(D)=-\\sum_{k=1}^{C}p_k \\cdot log_2(p_k)$$\n",
    "\n",
    "$$\\text{InfoGain}(D, a)=\\text{Entropy}(D)-\\sum_{v=1}^{V}\\frac{|D^v|}{|D|} \\cdot\\text{Entropy}(D^v)$$\n",
    "\n",
    "请完成以下三个内容：\n",
    "\n",
    "- 请自定义函数 cal_entropy(data, feature_name)计算数据集data关于feature_name的信息熵。输入参数 data 为 DataFrame，feature_name 为目标特征(或类别)的名称；\n",
    "\n",
    "- 请调用 cal_entropy() 函数计算决策树分支之前的信息熵，保存为 data_entropy；\n",
    "\n",
    "- 请自定义函数 cal_infoGain(data, base_entropy) 计算 weekend.txt 中各个特征的信息增益，保存为列表 infogains，并选择信息增益最大的分支节点 best_feature。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 补全代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-20T15:24:07.395329Z",
     "iopub.status.busy": "2022-10-20T15:24:07.394180Z",
     "iopub.status.idle": "2022-10-20T15:24:07.424050Z",
     "shell.execute_reply": "2022-10-20T15:24:07.423254Z",
     "shell.execute_reply.started": "2022-10-20T15:24:07.395290Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息增益列表： [0.0076, 0.0076, 0.0322, 0.0868]\n",
      "\n",
      "最优的分支节点名称： weather\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "weekend_data = pd.read_table('weekend.txt', sep=' ')\n",
    "#print(weekend_data)\n",
    "#print(weekend_data.shape)\n",
    "#hasChild = weekend_data['hasChild'].value_counts()\n",
    "#print(hasChild)\n",
    "#print(len(hasChild)) #2\n",
    "'''for index in range(len(hasChild)):\n",
    "        \n",
    "        ## 获取具体的取值频数freq\n",
    "        fre = hasChild[index]\n",
    "        numb = weekend_data.shape[0]\n",
    "        ## 通过频数计算类别概率prob，计算entropy\n",
    "        ## 请在下方补全代码\n",
    "        pro = fre/numb\n",
    "        entro = np.sum(pro*np.log2(pro)*(-1))\n",
    "print(round(entro, 3)) 计算hasChild的entropy'''\n",
    "\n",
    "#print(weekend_data.value_counts())\n",
    "#print(weekend_data.ndim)\n",
    "#print(list(weekend_data.columns.values)) #获取列名\n",
    "## 自定义计算entropy的函数\n",
    "def cal_entropy(data, feature_name):\n",
    "    '''\n",
    "    data : 数据集变量，DataFrame类型\n",
    "    featue_name : 目标特征名称\n",
    "    '''\n",
    "    \n",
    "    ## 声明数据集的熵\n",
    "    entropy = 0\n",
    "    \n",
    "    ## 获取data的样本数num\n",
    "    num = data.shape[0]\n",
    "    \n",
    "    ## 使用value_counts()函数获取目标特征`feature_name`取值的频数统计信息freq_stats\n",
    "    freq_stats = data[feature_name].value_counts()\n",
    "    \n",
    "    ## 遍历目标特征的不同取值频数,计算entropy\n",
    "    for index in range(len(freq_stats)):\n",
    "        \n",
    "        ## 获取具体的取值频数freq\n",
    "        freq = freq_stats[index]\n",
    "        \n",
    "        ## 通过频数计算类别概率prob，计算entropy\n",
    "        ## 请在下方补全代码\n",
    "        prob = freq / num\n",
    "        #entropy = np.sum(prob*np.log2(prob)*(-1))\n",
    "        entropy = entropy - prob * np.log2(prob)\n",
    "        \n",
    "\n",
    "\n",
    "    ## 返回结果\n",
    "    return round(entropy, 3)\n",
    "\n",
    "## 调用cal_entropy函数，计算weekend_data关于周末是否出门的信息熵\n",
    "data_entropy = cal_entropy(weekend_data,'status')\n",
    "#print(data_entropy)\n",
    "## 自定义计算信息增益的函数cal_infoGain\n",
    "## data: 数据集变量，DataFrame类型\n",
    "## base_entropy: 数据集的熵\n",
    "def cal_infoGain(data, base_entropy):\n",
    "    \n",
    "    ## 声明数据集特征的信息增益列表\n",
    "    infogain_list = []\n",
    "    \n",
    "    ## 获取数据集的样本数nums, 维度dims\n",
    "    nums = data.shape[0]\n",
    "    dims = data.ndim #为什么要计算维度\n",
    "    \n",
    "    ## 获取数据集的特征名称，类型为list\n",
    "    feature_list = list(data.columns.values)\n",
    "    ## 移除目标特征名称\n",
    "    feature_list.remove('status')\n",
    "    #print(feature_list)\n",
    "    ## 遍历每个特征\n",
    "    for feature in feature_list:\n",
    "        \n",
    "        ## 保存feature不同取值的加权熵\n",
    "        sub_entropy = 0\n",
    "        \n",
    "        ## 切片数据集，获取特征feature的数据记录feature_data \n",
    "        feature_data = data[feature]\n",
    "        \n",
    "        ## 使用value_count()函数获取特征feature取值的统计信息freq_stats\n",
    "        freq_stats = feature_data.value_counts()\n",
    "        #print(freq_stats)\n",
    "         \n",
    "        ## 计算信息增益\n",
    "        ## 请在下方补全代码\n",
    "        for i in range(len(freq_stats)):\n",
    "            #计算该特征每个取值的频数\n",
    "            prob_stats_sub = freq_stats[i]/nums \n",
    "            #计算第i个该特征的熵\n",
    "            feature_data_sub = data[feature_data==i]\n",
    "            feature_data_entropy = cal_entropy(feature_data_sub,'status')\n",
    "            #计算该特征不同取值的加权熵\n",
    "            sub_entropy = sub_entropy+prob_stats_sub*feature_data_entropy\n",
    "    \n",
    "        #print(prob_stats)\n",
    "        #print(cal_entropy(data, feature))\n",
    "        infogain = data_entropy - sub_entropy\n",
    "        \n",
    "\n",
    "\n",
    "        ## infogain取值保留小数点后4位，保存到infogain_list中\n",
    "        infogain_list.append(round(infogain, 4))\n",
    "    \n",
    "    ## 获取infogain_list的最大值所在的位置索引max_index\n",
    "    ## 根据max_index在feature_list中找到特征的名称best_feature\n",
    "    ## 请在下方补全代码\n",
    "    max_value = max(infogain_list)\n",
    "    max_index = infogain_list.index(max_value)\n",
    "    best_feature = feature_list[max_index]\n",
    "\n",
    "\n",
    "    \n",
    "    ## 返回结果\n",
    "    return infogain_list, best_feature\n",
    "\n",
    "## 调用cal_infogain()计算各个特征的信息增益infogains，并获取最优的分支节点名称best_feature\n",
    "infogains, best_feature = cal_infoGain(weekend_data, data_entropy)\n",
    "\n",
    "## 打印\n",
    "print (u'信息增益列表：', infogains)\n",
    "print ('')\n",
    "print (u'最优的分支节点名称：', best_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 期望输出：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3d80a48b1b80443eae424c908401e885ea91d3cb4d3a45d698f55e4df46d84fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务2：常见的决策树算法\n",
    "\n",
    "现在有一份有关商品销量的数据集product.csv，数据集的离散型特征信息如下：\n",
    "\n",
    "|特征名称|\t取值说明|\n",
    "| -------- | -------- |\n",
    "| 天气\t| 1：天气好；0：天气坏| \n",
    "| 是否周末\t| 1：是；0：不是| \n",
    "| 是否有促销| \t1：有促销；0：没有促销| \n",
    "| 销量| \t1：销量高；0：销量低| \n",
    "\n",
    "请完成以下三个内容：\n",
    "- 请根据提供的商品销量数据集 data，使用 sklearn 中的 DecisionTreeClassifier()函数构建决策树模型，模型选择分支结点的特征以Gini指数为判定准则；\n",
    "- 训练模型，并对测试集test_X进行预测，将预测结果存为 pred_y，进行模型评估；\n",
    "- 将构建的决策树模型进行可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 补全代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-20T15:24:07.426145Z",
     "iopub.status.busy": "2022-10-20T15:24:07.425561Z",
     "iopub.status.idle": "2022-10-20T15:24:07.757058Z",
     "shell.execute_reply": "2022-10-20T15:24:07.755725Z",
     "shell.execute_reply.started": "2022-10-20T15:24:07.426102Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.75      0.73         4\n",
      "weighted avg       0.83      0.75      0.73         4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'graphviz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_192/1761895420.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                 \u001b[0;34m,\u001b[0m\u001b[0mrounded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                )\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graphviz' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data = pd.read_csv('product.csv')\n",
    "#print(data)\n",
    "## 对数据集切片，获取除目标特征以外的其他特征的数据记录X\n",
    "X = data.drop('销量',axis=1)#小括号 指明是列，默认是行\n",
    "#print(X)\n",
    "\n",
    "## 对数据集切片，获取目标特征`销量`的数据记录y\n",
    "y = data.loc[:,'销量'] #注意写法\n",
    "#print(y)\n",
    "## 使用train_test_split函数划分训练集train_X, train_y和测试集test_X, test_y\n",
    "## 测试集所占比例为0.1,random_state为0\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = .1, random_state = 0)\n",
    "\n",
    "## 构建分支节点选择方法为基尼指数的决策树模型tree_model，进行模型训练、测试与性能评估\n",
    "## 请在下方补全代码\n",
    "tree_model = DecisionTreeClassifier(criterion='gini',random_state=0)\n",
    "tree_model = tree_model.fit(train_X, train_y)\n",
    "pred = tree_model.predict(test_X)\n",
    "print(classification_report(test_y,pred))\n",
    "\n",
    "## 决策树可视化\n",
    "from sklearn.tree import export_graphviz \n",
    "#import graphviz 改了一下，不然报错\n",
    "dot_data = export_graphviz(tree_model\n",
    "                                ,out_file=None\n",
    "                                ,feature_names= [\"天气\",\"周末\",\"促销\"]\n",
    "                                ,class_names=[\"销量低\",\"销量高\"]\n",
    "                                ,filled=True\n",
    "                                ,rounded=True\n",
    "                               )\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 期望输出：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f7c9f4d97660416b9ac354a1bcd6c87efcb7a0958cfa4579bf70a83d01ee64f7)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/eb46fe19bf43414290f904042a511f25140e1908a2eb4c2e81c52450f1de68bd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
